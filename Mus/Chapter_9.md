[ToC]

## 9장 - 웹 크롤러 설계
**크롤러 사용 예**
- 검색 엔진 인덱싱(search engine indexing)
- 웹 아카이빙(web archiving)
- 웹 마이닝(web mining) -> 저 이거 비슷한거 알바로 해봤습니다!
- 웹 모니터링(web monitoring)

### 1단계 - 문제 이해 및 설계 범위 확장
**웹 크롤러의 기본 알고리즘**
1. URL 집합이 입력으로 주어지면, 해당 URL 모든 웹페이지 다운로드
2. 다시 URL 추출
3. 추출된 URL에서 다운로드 URL 목록 추가 후 위과정 반복

- 크롤러가 만족해야할 속성
  - 규모 확장성
  - 안정성
  - 예절 -> 너무 귀엽잖아!
  - 확장성

#### 개략적 규모 추정

### 2단계 - 걔략적 설계안 제시 및 동의 구하기
#### 시작 URL 집합
- 크롤링을 시작하는 출발점
- 알고리즘을 잘 골라야 하는데 일반적으로 전체 URL 공간을 작은 부분집합으로 나누는 전략
  - 지역적
  - 주제별

#### 미수집 URL 저장소
- 크롤링 상태
  - 다운로드할 URL
    - 저장 관리: URL 저장소(URL frontier)
  - 다운로드된 URL

#### HTML 다운로더
- 인터넷에서 웹 페이지를 다운로드

#### 도메임 이름 변환기
- URL을 IP로 변환

#### 콘텐츠 파서
- 이상한 웹페이지를 거르기 위해 파싱과 검증을 함

#### 중복 콘텐츠인가?
- 연구 결과에 따르면 웹페이지 중복이 29%
- 중복을 줄이고 소요되는 시간을 줄임

#### 콘텐츠 저장소
- HTML 문서를 보관하는 시스템
- 데이터 양이 너무 많으므로 디스크에 저장
- 인기 있는 콘텐츠는 메모리에 두어 지연시간 줄임

#### URL 추출기
- HTML 페이지를 파싱하여 링크들을 골라냄
- 상대경로는 전부 절대 경로로 변환

#### URL 필터
- 접근 제외 목록에 포함된 URL 등을 배제

#### 이미 방문한 URL?
- 서버 부하를 줄익 시스템이 무한 루프에 빠지는 일을 방지

#### URL 저장소
- 이미 방문한 URL 보관

#### 웹 크롤러 작업 흐름
1. 시작 URL을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저자장소에서 URL 목록을 가져온다
3. 도메인 이름 변화기를 사용하여 URL의 IP를 알아내고 웹페이지를 다운
4. 콘텐츠 파서가 올바른 형식을 갖춘 페이지인지 검증
5. 중복 콘텐츠인지 확인 절차 개시
6. 중복 확인
  - 이미 저장소에 있는 콘텐츠는 버림
  - 저장소에 없으면 저장소에 저장 뒤 URL 추출기로 전달
7. HTML 페이지에서 링크 추출
8. URL 필터로 전달
9. 중복 URL 판별
10. URL 저장소에서 이미 처리된 URL 확인
11. 저장소에 없는 URL은 저장후 미수집 URL 저장소에 전달

### 3단게 - 상세 설계
- DFS(Depth-First Search) vs BFS(Breadth-First Search)
- 미수집 URL 저장소
- HTML 다운로더
- 안정성 확보 전략
- 확장성 확보 전략

#### DFS를 쓸 것인가, BFS를 쓸것인가
- DFS는 깊이가 깊을수록 가늠하기 어렵기 때문에 좋지 않을 수 있음
- 보통 BFS를 씀
- BFS 큰 문제점 2개
  - 한 페이지에서 나오는 상당수는 같은 서버로 돌아감
  - 표준적 BFS 알고리즘은 URL 간에 우선순위가 없음
    - 우선순위가 있어야 함!

#### 미수집 URL 저장소
- 예의!!!!!!
- 막 요청하면 예의 없음!!!
  - 큐 라우터: 같은 호스트에 속한 URL은 같은 큐로
  - 매핑 테이블
  - FIFO 큐
  - 큐 선택기: 큐들을 순회하면서 큐의 URL을 작업 스레드로 전달
  - 작업 스레드: 전달된 URL을 당운로드
- 우선순위
  - 중요한 페이지 우선
  - 페이지 랭크
  - 트래픽 양
  - 갱신 빈도
  - 순위 결정장치: 우선순위를 계산
  - 큐: 유선순의별로 큐가 하나씩 할당
  - 큐 선택기: 임의 큐에서 UIRL을 꺼내는 역할
  - 전면 큐: 유선순의 결정 과정 처리
  - 후면 큐: 크롤러야... 예의있게 굴어야지...
- 신선도
  - 재수집하는 양을 줄임
  - 웹페이지의 변경 이력을 활용
  - 운선순위 활용하여 중요한 페이지는 좀 더 재수집
- 미수집 URL 저장소를 위한 지속성 저장장치
  - 대부분의 URL은 디스크에 두지만 IO(인풋 아웃풋이겠죠?) 비용을 줄이기 위해 메모리 버퍼에 큐를 둠
  - 버퍼에 있는 데이터는 주기적으로 디스크에 기록

#### HTML 다운로더
- HTTP 프로토콜을 통해 웹 페이지를 내려 받는다
  - HTTPS는 다른 프로토콜인데 이건 어떻게? 그냥 되려나??
- Robots.txt
  - 웹사이트가 크롤러와 소통하는 표준적 방법
- 성능 최적화
  1. 분산 크롤링
  2. 도메인 이름 변환 결과 캐시
    - 도메인 이름 변환기(DNS Resolver -> 이걸 왜케 어렵게 부르지..)
      - 크롤러 성능의 병목 -> 동기적 특성 때문
      - DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해 놓고 크론 잡을 통해 성능을 개선
  3. 지역성
  4. 짧은 타임아웃: 오래 기다리지 않는다.

#### 안정성
- 안정해시
- 크롤링 상태 및 수집 데이터 저장: 장애시 빠르게 복구
- 예외 처리
- 데이터 검증
  - 어떤식으로 데이터를 검증할 수 있을까요??


#### 확장성
- 이미지 다운로드 기능 추가!!!
- 웹 모니터를 통해 문제 사전 방지!!

#### 문제 있는 콘텐츠 감지 및 회피
1. 중복 콘텐츠
2. 거미 덫
  - 무한루프에 빠트림
3. 데이터 노이즈
  - 스팸

#### 4단계 - 마무리
추가로... 항상 비슷한 내용 추카포카

## 함께 논의하고 싶은 주제
- 예전에 크롤링을 알바로 해봤던 적이 있는데, 저는 크롤링을 했던게 아니라 큐잉큐잉을 했었다는 것을 알았습니다. 뭔가 도니스나 라일리는 크롤링을 코인 관련해서 업무적으로 해보셨을 것 같은데 어떻게 구현하셨는지 궁금합니다.

