# chapter 9 웹 크롤러 설계

웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것

몇개의 웹 페이지를 시작하여 그 링크를 따라 나가면서 새로운 콘텐츠를 수집

웹 크롤러의 다양한 용도
- 검색 엔진 인덱싱: 보편적 용례, 웹페이지를 모아 검색 엔진을 위한 로컬 인덱스 만듬
- 웹 아카이빙: 장기보관하기 위해 웹에서 정보를 모으는 절차
- 웹 마이닝: 인터넷에서 유용한 지식들을 도출 가능
- 웹 모니터링: 인터넷에서 저작권, 상표권이 침해되는 사례를 모니터링 가능

### 웹 크롤러의 기본 알고리즘
1. URL 집합 입력 -> URL이 가리키는 모든 웹 페이지 다운
2. 다운받은 웹 페이지에서 URL 추출
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위 과정 반복

### 웹 크롤러가 만족시켜야할 속성
- 규모 확장성
- 안정성: 비정상적 입력이나 환경에 대응 필요
- 예절: 수집 대상 웹 사이트에 짧은 시간 안에 너무 많은 요청을 보내서는 안된다
- 확장성: 새로운 형태의 콘텐츠 추가가 쉬워야 한다

## 웹 크롤러 작업 흐름
1. 시작 URL 들은 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 목록을 가져오고 도메인 이름 변환기를 사용해 IP주소로 접속
3. 콘텐츠 파서는 HTML 파싱하여 올바른 페이지인지 검증
4. 해당 페이지가 저장소에 있는지 중복 확인(있는 경우-처리하지 않고 버림 / 없는 경우 저장소에 저장 및 URL 추출기로 전달)
5. URL 추출기는 HTML페이지에서 해당 링크를 골라내어 URL 필터로 전달
6. 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달
7. 이미 처리한 URL인지 중복 확인
8. ... 반복

## 상세 설계

### DFS vs BFS
- DFS 알고리즘: 깊이 우선 탐색법
  - 그래프 크기가 클 경우 어느정도 깊숙이 가게될지 가늠이 어렵다
- BFS 알고리즘: 너비 우선 탬색법
  - 웹 크롤러는 보텅 이 방법을 사용
  - FIFO 큐를 사용하는 알고리즘
  - 한쪽으로는 탐색할 URL을 넣고, 다른 한쪽으로는 꺼내기만
  - URL 간에 우선순위를 두지 않는다 -> 처리 순서에 있는 모든 페이지를 공평하게 대우

### 미수집 URL 저장소

다운로드 할 URL을 보관하는 장소

저장소 구현 방법
- 예의
  - 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 무례하게 보내지 않는 것
  - 동일한 웹사이트는 한 번에 한 페이지만 요청
  - 시간차를 두고 다운 실행
  - 호스트명과 다운로드를 실행하는 작업 스레드 사이의 관계 유지 필요
  
- 우선순위
  - 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도를 이용
  - 큐 선택기: 임의 큐에서 처리할 URL을 꺼내는 역할, 순위가 높은 큐에서 더 자주 꺼내도록 함

- 신선도
  - 데이터의 신섬함을 위해 이미 다운로드한 페이지라고 해도 재수집할 필요 있음
  - 웹 페이지의 변경 이력 활용
  - 우선순위 활용하여 중요한 페이지는 더 자주 재수집


### 추가적으로 논의해보면 좋을 것
서버측 렌더링, 원치 않는 페이지 필터링, 데이터 베이스 다중화 및 샤딩, 수평적 규모 확장성, 데이터 분석 솔루션

# 논의주제
웹 크롤링을 통해 데이터 분석이 필요한 예시가 뭐가 있을까요? 우리 서비스로 생각한다면, SNS 서비스에 대한 사용자들의 생각, AR 서비스에 대한 정보 등이 있을까요? 